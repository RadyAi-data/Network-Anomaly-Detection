{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "902671c6",
   "metadata": {},
   "source": [
    "In this project I will use Unsuprvised model (Isolation forest) and Supervised model (choosen later).\n",
    "\n",
    "Isolation forest needs all the normal data and not labeled. \n",
    "\n",
    "Supervised will need only the anomlies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b51edd87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Isolation Forest training dataset ready and saved!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sqlalchemy import create_engine\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# --- 1. Connect to PostgreSQL ---\n",
    "load_dotenv()\n",
    "\n",
    "user = os.getenv(\"POSTGRES_USER\")\n",
    "password = os.getenv(\"POSTGRES_PASSWORD\")\n",
    "host = os.getenv(\"POSTGRES_HOST\")\n",
    "port = os.getenv(\"POSTGRES_PORT\")\n",
    "db = os.getenv(\"POSTGRES_DB\")\n",
    "\n",
    "engine = create_engine(f\"postgresql://{user}:{password}@{host}:{port}/{db}\")\n",
    "connection = engine.connect()\n",
    "\n",
    "# --- 2. Load only NORMAL traffic for Isolation Forest ---\n",
    "query_normal = \"\"\"\n",
    "SELECT *\n",
    "FROM traffic\n",
    "WHERE label = 'normal'\n",
    "\"\"\"\n",
    "df_normal = pd.read_sql(query_normal, connection)\n",
    "\n",
    "# --- 3. Identify feature types ---\n",
    "categorical_cols = ['protocol_type', 'service', 'flag']\n",
    "numerical_cols = df_normal.drop(columns=['label', 'difficulty_level'] + categorical_cols).columns.tolist()\n",
    "\n",
    "# --- 4. Build preprocessing pipeline ---\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_cols),\n",
    "        ('cat', OneHotEncoder(sparse_output=False, drop='first'), categorical_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# --- 5. Apply transformations ---\n",
    "X_normal = preprocessor.fit_transform(df_normal)\n",
    "\n",
    "# --- 6. Convert back to DataFrame for inspection or saving ---\n",
    "# Get one-hot column names\n",
    "ohe_cols = preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_cols)\n",
    "all_cols = numerical_cols + list(ohe_cols)\n",
    "df_normal_processed = pd.DataFrame(X_normal, columns=all_cols)\n",
    "\n",
    "# --- 7. Save for later ---\n",
    "df_normal_processed.to_csv(\"../data/isolation_forest_train.csv\", index=False)\n",
    "print(\"Isolation Forest training dataset ready and saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac1dbc7",
   "metadata": {},
   "source": [
    "Use grouping in Supervised model data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2196bb83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attack_group\n",
      "DoS      31480\n",
      "Probe     8159\n",
      "R2L        696\n",
      "Other      669\n",
      "U2R         37\n",
      "Name: count, dtype: int64\n",
      "Anomalies saved to CSV!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load .env\n",
    "load_dotenv()\n",
    "\n",
    "# DB connection variables\n",
    "user = os.getenv(\"POSTGRES_USER\")\n",
    "password = os.getenv(\"POSTGRES_PASSWORD\")\n",
    "host = os.getenv(\"POSTGRES_HOST\")\n",
    "port = os.getenv(\"POSTGRES_PORT\")\n",
    "db = os.getenv(\"POSTGRES_DB\")\n",
    "\n",
    "# Connect\n",
    "engine = create_engine(f\"postgresql://{user}:{password}@{host}:{port}/{db}\")\n",
    "connection = engine.connect()\n",
    "\n",
    "# --- Query only anomaly rows ---\n",
    "query_anomalies = \"\"\"\n",
    "SELECT *\n",
    "FROM traffic\n",
    "WHERE label != 'normal'\n",
    "\"\"\"\n",
    "df_anomalies = pd.read_sql(query_anomalies, connection)\n",
    "\n",
    "# --- Map attack groups ---\n",
    "attack_groups = {\n",
    "    'DoS': ['neptune', 'smurf', 'teardrop', 'pod', 'land'],\n",
    "    'Probe': ['satan', 'ipsweep', 'portsweep', 'nmap'],\n",
    "    'R2L': ['guess_passwd', 'ftp_write', 'imap', 'phf', 'warezclient', 'multihop', 'warezmaster'],\n",
    "    'U2R': ['buffer_overflow', 'loadmodule', 'rootkit', 'perl', 'spy']\n",
    "}\n",
    "\n",
    "def map_attack_group(label):\n",
    "    for group, attacks in attack_groups.items():\n",
    "        if label in attacks:\n",
    "            return group\n",
    "    return 'Other'  # fallback, just in case\n",
    "\n",
    "# Apply mapping\n",
    "df_anomalies['attack_group'] = df_anomalies['label'].apply(map_attack_group)\n",
    "\n",
    "# Check counts\n",
    "print(df_anomalies['attack_group'].value_counts())\n",
    "import os\n",
    "os.makedirs('../results', exist_ok=True)\n",
    "\n",
    "# Save df_anomalies to CSV\n",
    "df_anomalies.to_csv('../data/anomalies_grouped.csv', index=False)\n",
    "print(\"Anomalies saved to CSV!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bca63fc",
   "metadata": {},
   "source": [
    "Downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "50c3465d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attack_group\n",
      "DoS      31480\n",
      "Probe     8159\n",
      "R2L        696\n",
      "Other      669\n",
      "U2R         37\n",
      "Name: count, dtype: int64\n",
      "attack_group\n",
      "DoS      11480\n",
      "Probe     8159\n",
      "R2L        696\n",
      "U2R         37\n",
      "Name: count, dtype: int64\n",
      "Balanced dataset saved to ../data/balanced_anomalies.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Check counts\n",
    "print(df_anomalies['attack_group'].value_counts())\n",
    "\n",
    "# Separate each group\n",
    "dos = df_anomalies[df_anomalies['attack_group'] == 'DoS']\n",
    "probe = df_anomalies[df_anomalies['attack_group'] == 'Probe']\n",
    "r2l = df_anomalies[df_anomalies['attack_group'] == 'R2L']\n",
    "u2r = df_anomalies[df_anomalies['attack_group'] == 'U2R']\n",
    "other = df_anomalies[df_anomalies['attack_group'] == 'Other']\n",
    "\n",
    "# Decide on a target number per class (for example, match Probe's size ~8k)\n",
    "target_count = 11480\n",
    "\n",
    "# Downsample DoS\n",
    "dos_downsampled = dos.sample(n=target_count, random_state=42)\n",
    "\n",
    "# Downsample Other (optional, to keep small)\n",
    "#probe_downsampled = probe.sample(n=target_count, random_state=42)\n",
    "\n",
    "# Keep the smaller classes as-is (Probe, R2L, U2R)\n",
    "df_balanced = pd.concat([dos_downsampled, probe, r2l, u2r], axis=0)\n",
    "\n",
    "# Shuffle the dataset\n",
    "df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Check new distribution\n",
    "print(df_balanced['attack_group'].value_counts())\n",
    "\n",
    "output_path = '../data/balanced_anomalies.csv'  \n",
    "df_balanced.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Balanced dataset saved to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
